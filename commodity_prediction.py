{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":94771,"databundleVersionId":13044405,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-28T10:50:35.828665Z\",\"iopub.execute_input\":\"2025-08-28T10:50:35.829342Z\",\"iopub.status.idle\":\"2025-08-28T10:50:37.126575Z\",\"shell.execute_reply.started\":\"2025-08-28T10:50:35.829311Z\",\"shell.execute_reply\":\"2025-08-28T10:50:37.125810Z\"}}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-28T10:50:37.512429Z\",\"iopub.execute_input\":\"2025-08-28T10:50:37.512863Z\",\"iopub.status.idle\":\"2025-08-28T10:50:38.235340Z\",\"shell.execute_reply.started\":\"2025-08-28T10:50:37.512840Z\",\"shell.execute_reply\":\"2025-08-28T10:50:38.234438Z\"}}\ntrain = pd.read_csv(\"/kaggle/input/mitsui-commodity-prediction-challenge/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/mitsui-commodity-prediction-challenge/test.csv\")\ntrain_labels = pd.read_csv(\"/kaggle/input/mitsui-commodity-prediction-challenge/train_labels.csv\")\ntarget_pairs = pd.read_csv(\"/kaggle/input/mitsui-commodity-prediction-challenge/target_pairs.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:34.946513Z\",\"iopub.execute_input\":\"2025-08-27T20:58:34.946874Z\",\"iopub.status.idle\":\"2025-08-27T20:58:34.998711Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:34.946840Z\",\"shell.execute_reply\":\"2025-08-27T20:58:34.997511Z\"}}\ntrain.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.001292Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.003111Z\",\"iopub.status.idle\":\"2025-08-27T20:58:35.031216Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.003070Z\",\"shell.execute_reply\":\"2025-08-27T20:58:35.029704Z\"}}\ntrain.isna().sum()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.032092Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.032576Z\",\"iopub.status.idle\":\"2025-08-27T20:58:35.433177Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.032541Z\",\"shell.execute_reply\":\"2025-08-27T20:58:35.432083Z\"}}\nnan_columns = train.columns[train.isna().any()].tolist()\nprint(\"Columns with NaNs:\", nan_columns)\n\ntrain[nan_columns] = train[nan_columns].interpolate(method='linear', axis=0)\n\ntrain[nan_columns] = train[nan_columns].ffill().bfill()\n\nprint(train[nan_columns].isna().sum())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.434282Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.434726Z\",\"iopub.status.idle\":\"2025-08-27T20:58:35.635500Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.434701Z\",\"shell.execute_reply\":\"2025-08-27T20:58:35.634480Z\"}}\nnan_columns_test = test.columns[test.isna().any()].tolist()\nprint(\"Columns with NaNs:\", nan_columns)\n\ntest[nan_columns_test] = test[nan_columns].interpolate(method='linear', axis=0)\n\ntest[nan_columns] = test[nan_columns].ffill().bfill()\n\nprint(test[nan_columns].isna().sum())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.636375Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.636668Z\",\"iopub.status.idle\":\"2025-08-27T20:58:35.872719Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.636643Z\",\"shell.execute_reply\":\"2025-08-27T20:58:35.871702Z\"}}\nnan_columns_labels = train_labels.columns[train_labels.isna().any()].tolist()\nprint(\"Columns with NaNs:\", nan_columns_labels)\n\ntrain_labels[nan_columns_labels] = train_labels[nan_columns_labels].interpolate(method='linear', axis=0)\n\ntrain_labels[nan_columns_labels] = train_labels[nan_columns_labels].ffill().bfill()\n\nprint(train_labels[nan_columns_labels].isna().sum())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.873789Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.874127Z\",\"iopub.status.idle\":\"2025-08-27T20:58:35.881843Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.874093Z\",\"shell.execute_reply\":\"2025-08-27T20:58:35.880660Z\"}}\nlme_cols = [col for col in train.columns if col.startswith(\"LME_\")]\njpx_cols = [col for col in train.columns if col.startswith(\"JPX_\")]\nus_cols  = [col for col in train.columns if col.startswith(\"US_\")]\nfx_cols  = [col for col in train.columns if col.startswith(\"FX_\")]\n\n\nmarket_groups = {\n    \"LME\": lme_cols,\n    \"JPX\": jpx_cols,\n    \"US\": us_cols,\n    \"FX\": fx_cols\n}\n\n\nfor k, v in market_groups.items():\n    print(f\"{k}: {len(v)} features\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:35.882975Z\",\"iopub.execute_input\":\"2025-08-27T20:58:35.883377Z\",\"iopub.status.idle\":\"2025-08-27T20:58:39.414995Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:35.883353Z\",\"shell.execute_reply\":\"2025-08-27T20:58:39.414038Z\"}}\nexclude = {\"date_id\"}\nfeat_cols = [c for c in train.columns if c not in exclude and any(c.startswith(p+\"_\") for p in market_groups)]\n\ncorr_all = train[feat_cols].corr()  # Pearson by default\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_all, cmap=\"coolwarm\", center=0)\nplt.title(\"Test Heatmap ‚Äì No Mask\")\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:39.417504Z\",\"iopub.execute_input\":\"2025-08-27T20:58:39.417793Z\",\"iopub.status.idle\":\"2025-08-27T20:58:39.834904Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:39.417771Z\",\"shell.execute_reply\":\"2025-08-27T20:58:39.833523Z\"}}\n# Flatten the correlation matrix\ncorr_pairs = corr_all.unstack().dropna()\n\n# Drop self-correlations (always 1.0)\ncorr_pairs = corr_pairs[corr_pairs != 1]\n\n# Keep only absolute correlations above 0.7\nstrong_corrs = corr_pairs[abs(corr_pairs) > 0.7]\n\n# Sort descending\nstrong_corrs = strong_corrs.sort_values(ascending=False)\n\nprint(strong_corrs)\n# Keep only features that appear in strong correlations\nstrong_feats = list(set([i for i,j in strong_corrs.index] + [j for i,j in strong_corrs.index]))\n\n# Subset the original correlation matrix\nstrong_corr_matrix = corr_all.loc[strong_feats, strong_feats]\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:39.836463Z\",\"iopub.execute_input\":\"2025-08-27T20:58:39.836799Z\",\"iopub.status.idle\":\"2025-08-27T20:58:39.861005Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:39.836760Z\",\"shell.execute_reply\":\"2025-08-27T20:58:39.860098Z\"}}\nstrong_corr_matrix\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:39.862122Z\",\"iopub.execute_input\":\"2025-08-27T20:58:39.862462Z\",\"iopub.status.idle\":\"2025-08-27T20:58:44.061246Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:39.862428Z\",\"shell.execute_reply\":\"2025-08-27T20:58:44.059448Z\"}}\ncross_pairs = []\nfor a_name, a_cols in market_groups.items():\n    for b_name, b_cols in market_groups.items():\n        if a_name >= b_name:  # avoid duplicates & same group\n            continue\n        sub = train[a_cols + b_cols].corr().loc[a_cols, b_cols].stack()\n        sub = sub.abs().sort_values(ascending=False)\n        # Keep top N for this pair of groups\n        topN = sub.head(20)\n        cross_pairs.append(\n            topN.rename(\"abs_corr\").reset_index().assign(group_a=a_name, group_b=b_name)\n        )\n\ntop_cross = pd.concat(cross_pairs, ignore_index=True)\nprint(top_cross.head(30))\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:44.062589Z\",\"iopub.execute_input\":\"2025-08-27T20:58:44.062863Z\",\"iopub.status.idle\":\"2025-08-27T20:58:49.336101Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:44.062841Z\",\"shell.execute_reply\":\"2025-08-27T20:58:49.335072Z\"}}\nfor name, cols in market_groups.items():\n    if len(cols) < 2:\n        continue\n    corr_g = train[cols].corr()\n    mask = np.triu(np.ones_like(corr_g, dtype=bool))\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_g, mask=mask, cmap=\"coolwarm\", center=0, linewidths=0.1)\n    plt.title(f\"Intra-Group Correlation ‚Äì {name}\")\n    plt.tight_layout()\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:49.337371Z\",\"iopub.execute_input\":\"2025-08-27T20:58:49.337740Z\",\"iopub.status.idle\":\"2025-08-27T20:58:56.052456Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:49.337707Z\",\"shell.execute_reply\":\"2025-08-27T20:58:56.051467Z\"}}\ndef compute_log_returns(df, cols):\n    returns = df[cols].apply(lambda x: np.log(x) - np.log(x.shift(1)))\n    return returns\n\nfor name, cols in market_groups.items():\n    if len(cols) == 0:\n        continue\n    plt.figure(figsize=(12, 6))\n    sns.histplot(compute_log_returns(train, cols).stack(), bins=50, kde=True)\n    plt.title(f\"{name} ‚Äì Distribution of Log Returns\")\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:56.053649Z\",\"iopub.execute_input\":\"2025-08-27T20:58:56.054001Z\",\"iopub.status.idle\":\"2025-08-27T20:58:57.822016Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:56.053973Z\",\"shell.execute_reply\":\"2025-08-27T20:58:57.820750Z\"}}\nfor name, cols in market_groups.items():\n    if len(cols) == 0:\n        continue\n    returns = compute_log_returns(train, cols)\n    if name in [\"JPX\",\"US\"] :\n        top_vol_cols = returns.std().sort_values(ascending=False).head(15).index\n        plt.figure(figsize=(12,6))\n        sns.boxplot(data=returns[top_vol_cols])\n        plt.title(f\"{name} ‚Äì Top 15 Most Volatile Instruments\")\n        plt.xticks(rotation=45)\n        plt.show()\n    else :\n        plt.figure(figsize=(12, 6))\n        sns.boxplot(data=returns)\n        plt.title(f\"{name} ‚Äì Boxplot of Log Returns\")\n        plt.xticks(rotation=90)\n        plt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:57.823446Z\",\"iopub.execute_input\":\"2025-08-27T20:58:57.823904Z\",\"iopub.status.idle\":\"2025-08-27T20:58:59.465193Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:57.823862Z\",\"shell.execute_reply\":\"2025-08-27T20:58:59.463833Z\"}}\nimport matplotlib.pyplot as plt\n\nwindow = 20  # 20-day rolling window\n\nfor name, cols in market_groups.items():\n    if len(cols) == 0:\n        continue\n    returns = compute_log_returns(train, cols)  # log returns\n    rolling_vol = returns.rolling(window).std()\n    \n    plt.figure(figsize=(12,6))\n    for col in rolling_vol.columns[:5]:  # plot only first 5 instruments per group for readability\n        plt.plot(rolling_vol[col], label=col)\n    \n    plt.title(f\"{name} ‚Äì Rolling {window}-day Volatility (Top 5 instruments)\")\n    plt.xlabel(\"Date Index\")\n    plt.ylabel(\"Volatility\")\n    plt.legend()\n    plt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:58:59.466242Z\",\"iopub.execute_input\":\"2025-08-27T20:58:59.466512Z\",\"iopub.status.idle\":\"2025-08-27T20:59:00.047843Z\",\"shell.execute_reply.started\":\"2025-08-27T20:58:59.466489Z\",\"shell.execute_reply\":\"2025-08-27T20:59:00.046753Z\"}}\nwindow = 20\ngroup_vol = {}\n\nfor name, cols in market_groups.items():\n    returns = compute_log_returns(train, cols)\n    group_vol[name] = returns.rolling(window).std().mean(axis=1)  # average volatility per group\n\nplt.figure(figsize=(10,6))\nfor name, vol in group_vol.items():\n    plt.plot(vol, label=name)\nplt.title(f\"Average Rolling {window}-day Volatility per Market Group\")\nplt.xlabel(\"Date Index\")\nplt.ylabel(\"Average Volatility\")\nplt.legend()\nplt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T20:59:00.048826Z\",\"iopub.execute_input\":\"2025-08-27T20:59:00.049104Z\",\"iopub.status.idle\":\"2025-08-27T21:03:04.757078Z\",\"shell.execute_reply.started\":\"2025-08-27T20:59:00.049083Z\",\"shell.execute_reply\":\"2025-08-27T21:03:04.756024Z\"}}\nfeature_target_corr = {}\nfeat_cols = [col for col in train.columns if col != 'date_id']\ntarget_cols = [col for col in train_labels.columns if col != 'date_id']\nlags = [0, 1, 2, 3]\nfor col in feat_cols:  # each instrument\n    for lag in lags:   # optional lagged features\n        feature_shifted = train[col].shift(lag)\n        # Loop over each target column\n        for target_col in target_cols:\n            feature_target_corr[(col, target_col, lag)] = feature_shifted.corr(train_labels[target_col])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-27T21:03:04.758107Z\"}}\ncorr_df = pd.DataFrame.from_dict(feature_target_corr, orient='index', columns=['correlation'])\ncorr_df = corr_df.sort_values(by='correlation', key=abs, ascending=False)\n\nprint(corr_df.head(20))  # top 20 correlations\n\n# %% [code]\ntop_n = 15  # number of top features to show per target\n\n# Loop over each target column\nfor target_col in target_cols[:5] :\n    # Filter correlations for this target\n    target_corrs = {k[0]: v for k, v in feature_target_corr.items() if k[1] == target_col and k[2] == 0}  # lag=0\n    \n    # Convert to Series and sort by absolute correlation\n    target_corrs_series = pd.Series(target_corrs)\n    target_corrs_series = target_corrs_series.reindex(target_corrs_series.abs().sort_values(ascending=False).index)\n\n    # Take top N\n    top_corrs = target_corrs_series.head(top_n)\n    \n    # Plot\n    plt.figure(figsize=(10,5))\n    top_corrs.abs().plot(kind='bar', color='skyblue')\n    plt.title(f\"Top {top_n} Correlated Instruments with Target: {target_col}\")\n    plt.ylabel(\"Absolute Correlation\")\n    plt.xlabel(\"Instrument\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n# %% [code]\ntop_targets = train_labels.var().sort_values(ascending=False).head(5).index\n\n# 2. Compute market correlation summary\nmarket_corr_summary = {}\n\nfor target_col in top_targets:  # only top targets\n    market_corr_summary[target_col] = {}\n    \n    for market_name, market_cols in market_groups.items():\n        # Compute correlations of each feature in the market with the target\n        corrs = [train[col].corr(train_labels[target_col]) for col in market_cols if col in train.columns]\n        \n        if len(corrs) > 0:\n            # Store mean of absolute correlations\n            market_corr_summary[target_col][market_name] = pd.Series(corrs).abs().mean()\n        else:\n            market_corr_summary[target_col][market_name] = 0.0\n\n# 3. Convert to DataFrame\nmarket_corr_df = pd.DataFrame(market_corr_summary)\n\n# 4. Plot\nplt.figure(figsize=(12, 6))\nmarket_corr_df.T.plot(kind=\"bar\", figsize=(12, 6), width=0.8, edgecolor=\"black\")\nplt.title(\"Average Absolute Market Correlations (Top 5 Targets)\", fontsize=14)\nplt.ylabel(\"Average |Correlation|\", fontsize=12)\nplt.xlabel(\"Target\", fontsize=12)\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Market\")\nplt.tight_layout()\nplt.show()\n\n# %% [code]\n!pip install optuna\n\n# %% [code]\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom tqdm import tqdm\nimport optuna\nimport json\nimport joblib\nimport os\n\n# -------------------------\n# Feature Engineering\n# -------------------------\n\ndef feature_engineering(Xr, Yr, target_cols, lags=[1,2,3], windows=[3,5,7], arima_order=(2,1,2)):\n    # 1. Lag features\n    for lag in lags:\n        for col in target_cols:\n            Xr[f\"{col}_lag{lag}\"] = Yr[col].shift(lag)\n\n    # 2. Rolling mean/std\n    for window in windows:\n        for col in target_cols:\n            Xr[f\"{col}_rollmean{window}\"] = Yr[col].shift(1).rolling(window=window).mean()\n            Xr[f\"{col}_rollstd{window}\"] = Yr[col].shift(1).rolling(window=window).std()\n\n    # 3. Interaction features\n    for i in range(len(target_cols)):\n        for j in range(i+1, len(target_cols)):\n            Xr[f\"{target_cols[i]}_x_{target_cols[j]}\"] = Yr[target_cols[i]] * Yr[target_cols[j]]\n\n    # 4. ARIMA features\n    arima_features = pd.DataFrame(index=Xr.index)\n    for col in tqdm(target_cols, desc=\"Building ARIMA features\"):\n        try:\n            model = ARIMA(Yr[col], order=arima_order)\n            fit = model.fit()\n            arima_features[f\"{col}_arima_trend\"] = fit.fittedvalues\n            arima_features[f\"{col}_arima_resid\"] = fit.resid\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Skipping {col} due to error: {e}\")\n\n    Xr = pd.concat([Xr, arima_features], axis=1)\n\n    # Fill NaNs\n    Xr = Xr.fillna(method=\"bfill\").fillna(method=\"ffill\")\n    return Xr\n\n\n# -------------------------\n# Applying feature engineering\n# -------------------------\n\nfeature_cols = [c for c in train.columns if c != \"date_id\"]\ntarget_cols = [c for c in train_labels.columns if c != \"date_id\"]\n\nXr = train[feature_cols].astype(\"float32\")\nYr = train_labels.astype(\"float32\")\nXt = test[feature_cols].astype(\"float32\")\n\nXr = feature_engineering(Xr, Yr, target_cols, lags=[1,2,3,4,5], windows=[3,5,10, 20], arima_order=(2,1,2))\n\n# -------------------------\n# Selecting top correlated features with mean target\n# -------------------------\n\nmean_target = Yr.mean(axis=1)\ncorrs = Xr.corrwith(mean_target).abs().sort_values(ascending=False)\nK = 100\ntop_features = corrs.head(K).index.tolist()\n\nXr = Xr[top_features]\nXt = Xt[top_features]  # same features in test\nwith open(\"/kaggle/working/top_features.json\", \"w\") as f:\n    json.dump(top_features, f)\n\n# -------------------------\n# Optuna hyperparameter tuning\n# -------------------------\n\ndef objective(trial, X, y):\n    params = {\n        \"objective\": \"regression\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.2),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.6, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.6, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-3, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-3, 10.0),\n        \"verbose\": -1,\n        \"n_jobs\": -1\n    }\n\n    tscv = TimeSeriesSplit(n_splits=3)\n    rmse_scores = []\n\n    for train_idx, val_idx in tscv.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = LGBMRegressor(**params)\n        model.fit(X_train, y_train)\n\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        rmse_scores.append(rmse)\n\n    return np.mean(rmse_scores)\n\n\n# -------------------------\n# Model training with tuned params\n# -------------------------\n\n\ntscv = TimeSeriesSplit(n_splits=5)\nrmse_scores = {}\nmodels = {}\n\nfor target_col in target_cols:  \n    print(f\"\\nüîé Hyperparameter tuning for target: {target_col}\")\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(lambda trial: objective(trial, Xr, Yr[target_col]), n_trials=25, show_progress_bar=True)\n\n    best_params = study.best_params\n    print(f\"‚úÖ Best params for {target_col}: {best_params}\")\n\n    # Train with best params\n    final_model = LGBMRegressor(**best_params)\n    final_model.fit(Xr, Yr[target_col])\n    models[target_col] = final_model\n    os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n    joblib.dump(final_model, f\"/kaggle/working/models/model_{target_col}.pkl\")\n    # CV evaluation\n    fold_rmse = []\n    for train_idx, val_idx in tscv.split(Xr):\n        X_train, X_val = Xr.iloc[train_idx], Xr.iloc[val_idx]\n        y_train, y_val = Yr.iloc[train_idx][target_col], Yr.iloc[val_idx][target_col]\n        preds = final_model.predict(X_val)\n        fold_rmse.append(np.sqrt(mean_squared_error(y_val, preds)))\n    \n    avg_rmse = np.mean(fold_rmse)\n    rmse_scores[target_col] = avg_rmse\n    print(f\"üìâ CV RMSE for {target_col}: {avg_rmse:.4f}\")\n\n    # Predict on test\n    y_test_pred = final_model.predict(Xt)\n    print(f\"Generated predictions for {target_col}\")\n\nprint(\"\\n‚úÖ Training + Tuning done.\")\nprint(\"CV RMSE summary:\", rmse_scores)","metadata":{"_uuid":"433b575e-bc44-4af4-b58b-3250115b8bf9","_cell_guid":"870b5d7e-3824-4f91-9d9f-c5cccac1e3ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}